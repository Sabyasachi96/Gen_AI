{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8585c1d-69e2-49ac-86d0-00eddc9d9e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n  Downloading pypdf-5.2.0-py3-none-any.whl.metadata (7.2 kB)\nCollecting databricks-vectorsearch\n  Downloading databricks_vectorsearch-0.46-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: mlflow-skinny<3,>=2.11.3 in /databricks/python3/lib/python3.12/site-packages (from databricks-vectorsearch) (2.15.1)\nCollecting protobuf==5.28.3 (from databricks-vectorsearch)\n  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nRequirement already satisfied: requests>=2 in /databricks/python3/lib/python3.12/site-packages (from databricks-vectorsearch) (2.32.2)\nCollecting deprecation>=2 (from databricks-vectorsearch)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from deprecation>=2->databricks-vectorsearch) (24.1)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (5.3.3)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (2.2.1)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (0.30.0)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (0.4)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (3.1.37)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (6.0.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (1.28.2)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (1.28.2)\nRequirement already satisfied: pytz<2025 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (2024.1)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (6.0.1)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (0.4.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch) (2024.6.2)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (2.21.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (3.17.0)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (1.2.15)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (0.49b2)\nRequirement already satisfied: typing-extensions>=3.7.4 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (4.11.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (1.14.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (5.0.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (4.9)\nRequirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (1.16.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch) (0.4.8)\nDownloading pypdf-5.2.0-py3-none-any.whl (298 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/298.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m298.7/298.7 kB\u001B[0m \u001B[31m13.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading databricks_vectorsearch-0.46-py3-none-any.whl (12 kB)\nDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/316.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m316.6/316.6 kB\u001B[0m \u001B[31m13.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: pypdf, protobuf, deprecation, databricks-vectorsearch\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 4.24.1\n    Not uninstalling protobuf at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-af457319-bcdf-435b-a63a-f5e7fdcd40f5\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-feature-engineering 0.8.0 requires protobuf<5,>=3.12.0, but you have protobuf 5.28.3 which is incompatible.\ntensorboard-plugin-profile 2.18.0 requires protobuf<5.0.0dev,>=3.19.6, but you have protobuf 5.28.3 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed databricks-vectorsearch-0.46 deprecation-2.1.0 protobuf-5.28.3 pypdf-5.2.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: sentence-transformers in /databricks/python3/lib/python3.12/site-packages (3.3.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (4.46.3)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (2.5.0+cpu)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (1.4.2)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (0.24.5)\nRequirement already satisfied: Pillow in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.5.0)\nRequirement already satisfied: packaging>=20.9 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (74.0.0)\nRequirement already satisfied: sympy==1.13.1 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /databricks/python3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /databricks/python3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: safetensors>=0.4.1 in /databricks/python3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.4)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mWarning: statements after `dbutils.library.restartPython()` will execute before Python is restarted.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf databricks-vectorsearch\n",
    "%pip install sentence-transformers\n",
    "dbutils.library.restartPython()  # Restart Python to apply changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87893b09-8b4c-4d10-ab2e-b45e67bda537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 09:03:32.685490: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-02-06 09:03:32.693010: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-02-06 09:03:32.733572: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-02-06 09:03:32.770687: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738832612.803573    2154 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738832612.813196    2154 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-02-06 09:03:32.855520: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "651e0284-1e35-4187-8c67-9954ca63da76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mantis shrimp\nTemporal range:\nOdontodactylus scyllarus (Red Mantis\nShrimp)\nLysiosquillina maculata (Zebra Mantis\nShrimp)\nScientiﬁc classiﬁcation\nDomain: Eukaryota\nKingdom: Animalia\nPhylum: Arthropoda\nClass: Malacostraca\nSubclass: Hoplocarida\nOrder: Stomatopoda\nLatreille, 1817\nSubdivisions[ 1 ] \nSuperfamilies and families\nBathysquilloidea\nBathysquillidae\nIndosquillidae\nGonodactyloidea\nAlainosquillidae\nHemisquillidae\nMantis shrimp\nMantis shrimp are carnivorous marine crustaceans of the\norder Stomatopoda (from Ancient Greek στόµα\n(stóma)  'mouth' and ποδός (podós)  'foot'). Stomatopods\nbranched off from other members of the class Malacostraca\naround 340 million years ago.[2] Mantis shrimp typically\ngrow to around 10 cm (3.9 in) in length, while a few can\nreach up to 38 cm (15 in).[3] A mantis shrimp's carapace\ncovers only the rear part of the head and the first four\nsegments of the thorax. Varieties range in colour from\nshades of brown to vivid colours, with more than 520 species\nof manti\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"/dbfs/mnt/lake/MantisShrimpFacts.pdf\" #path to your Mantis Shrimp facts PDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "    return text\n",
    "\n",
    "document_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(document_text[:1000])  # Show first 1000 characters for verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d6ca6a-e4a7-412a-8e2a-6ad79053ae7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 113\nSample Chunk:\n \n\nSample Chunk:\n Mantis shrimp\nTemporal range:\nOdontodactylus scyllarus (Red Mantis\nShrimp)\nLysiosquillina maculata (Zebra Mantis\nShrimp)\nScientiﬁc classiﬁcation\nDomain: Eukaryota\nKingdom: Animalia\nPhylum: Arthropoda\nClass: Malacostraca\nSubclass: Hoplocarida\nOrder: Stomatopoda\nLatreille, 1817\nSubdivisions[ 1 ] \nSuperfamilies and families\nBathysquilloidea\nBathysquillidae\nIndosquillidae\nGonodactyloidea\nAlainosquillidae\nHemisquillidae\nMantis shrimp\nMantis shrimp are carnivorous marine crustaceans of the\norder Stomatopoda (from Ancient Greek στόµα\n(stóma)  'mouth' and ποδός (podós)  'foot').\n\nSample Chunk:\n Stomatopods\nbranched off from other members of the class Malacostraca\naround 340 million years ago.[2] Mantis shrimp typically\ngrow to around 10 cm (3.9 in) in length, while a few can\nreach up to 38 cm (15 in).[3] A mantis shrimp's carapace\ncovers only the rear part of the head and the first four\nsegments of the thorax. Varieties range in colour from\nshades of brown to vivid colours, with more than 520 species\nof mantis shrimp known.\n\nSample Chunk:\n Retrieved May 13, 2020.\nDana Point Fish Company—Top and Bottom Views of Mantis Shrimp (http://www.danapointﬁsh\ncompany.com/mantis-shrimp/)\nTED talk (http://www.ted.com/talks/sheila_patek_clocks_the_fastest_animals)\nDeep Look (PBS) (https://www.youtube.com/watch?v=Lm1ChtK9QDU)\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Mantis_shrimp&oldid=1240362068\"\nExternal links.\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=512):\n",
    "    sentences = text.split(\". \")\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) < chunk_size:\n",
    "            chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence + \". \"\n",
    "    \n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Apply chunking\n",
    "chunks = chunk_text(document_text, chunk_size=512)\n",
    "\n",
    "# Display chunk information\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n",
    "print(\"Sample Chunk:\\n\", chunks[0])\n",
    "print()\n",
    "print(\"Sample Chunk:\\n\", chunks[1])\n",
    "print()\n",
    "print(\"Sample Chunk:\\n\", chunks[2])\n",
    "print()\n",
    "print(\"Sample Chunk:\\n\", chunks[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd7edf72-2333-40f3-b2a2-3fdef15bc5be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Only to be uncommented if first chunk is empty. Otherwise comment it. \n",
    "chunks = chunks[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc5c9b36-ecce-4c04-aa3a-27904622192d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document chunks stored in Delta Table (hive_metastore.rag_chunks)\n"
     ]
    }
   ],
   "source": [
    "# Convert chunks to Spark DataFrame\n",
    "spark.sql(\"USE hive_metastore\")  # Ensure we use the default catalog\n",
    "\n",
    "df = spark.createDataFrame([(i, chunk) for i, chunk in enumerate(chunks)], [\"id\", \"text\"])\n",
    "\n",
    "# Save to Delta Table in hive_metastore\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"rag_chunks\")\n",
    "\n",
    "print(\"Document chunks stored in Delta Table (hive_metastore.rag_chunks)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a6e2d69-428b-4a45-84a5-3778cf5885c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba1c94d5-bc6a-40ea-ac87-44b5c37c58cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text chunks stored in Delta Table.\n"
     ]
    }
   ],
   "source": [
    "# Now let's write the data into a delta live table.\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RAG\").getOrCreate()\n",
    "\n",
    "# Convert chunks to Spark DataFrame\n",
    "df = spark.createDataFrame([(i, chunk) for i, chunk in enumerate(chunks)], [\"id\", \"text\"])\n",
    "\n",
    "# Define Delta table path\n",
    "delta_table_path = \"/mnt/rag_delta_table\"\n",
    "\n",
    "# Save as Delta Table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "print(\"Text chunks stored in Delta Table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb3a1a7-c9b1-4a77-9e92-a4b721f4b205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01952f1e3c64e9e8015a41031bf6f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185665c3d4b04bfbacebfde5217be78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86055f195324afda39e0a358800093e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/90.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d80ad165f3c43a69affb186035b2a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54ed160bc444317944cfca4d77f928f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276a6c0958744d7fac84419f7bd635e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6df3e3d8c32442ea73bb6738f0a6fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0904667ab12c4584be2630ceb4e65165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676ade1a3b4740a7b84d0e0a23225044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba1dc5d6bab497f90914ab0a3f0875f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8142ab2158c9406cb4890ef8bc281f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings stored in Delta Table (hive_metastore.rag_embeddings)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load Hugging Face embedding model\n",
    "embedding_model = \"BAAI/bge-large-en\"\n",
    "model = SentenceTransformer(embedding_model)\n",
    "\n",
    "def generate_embedding(text):\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = [(i, generate_embedding(chunk)) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "embeddings_df = spark.createDataFrame(embeddings, [\"id\", \"embedding\"])\n",
    "\n",
    "# Save embeddings to Delta Table in hive_metastore\n",
    "embeddings_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"rag_embeddings\")\n",
    "\n",
    "print(\"Embeddings stored in Delta Table (hive_metastore.rag_embeddings)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6116e73-f81f-484f-9fe0-056beadf55c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Retrieved Chunks:\n******\nhas been observed to roll repeatedly for 2 m (6.6 ft), but specimens typically travel\nless than 1 m (3.3 ft).[51]\nThe mantis shrimp is eaten by a variety of cultures. In Japanese\ncuisine, the mantis shrimp species Oratosquilla oratoria,\ncalled shako ( 蝦蛄), is eaten boiled as a sushi topping, and\noccasionally raw as sashimi.\nMantis shrimps are also abundant along Vietnam's coast,\nknown in Vietnamese as bề bề, tôm tích or tôm tít.\n******\nAdditionally,\nsome rock-burrowing species can do more damage to live rock\nthan the fishkeeper would prefer.\nThe live rock with mantis shrimp burrows is considered useful\nby some in the marine aquarium trade and is often collected. A\npiece of live rock not uncommonly conveys a live mantis\nshrimp into an aquarium.\n******\nHowever, despite being\ncommon, they are poorly understood, as many species spend\nmost of their lives sheltering in burrows and holes.[4]\nCalled \"sea locusts\" by ancient Assyrians, \"prawn killers\" in\nAustralia,[5] and now sometimes referred to as \"thumb\nsplitters\" due to their ability to inflict painful wounds if\nhandled incautiously[6] mantis shrimp have powerful\nraptorial appendages that are used to attack and kill prey\neither by spearing, stunning, or dismembering.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Define a query embedding\n",
    "query_text = \"What does mantis shrimp eat?\"\n",
    "query_embedding = model.encode(query_text).tolist()\n",
    "\n",
    "# Load embeddings from Delta Table\n",
    "df_embeddings = spark.sql(\"SELECT * FROM rag_embeddings\").toPandas()\n",
    "\n",
    "# Compute similarity scores\n",
    "df_embeddings[\"similarity\"] = df_embeddings[\"embedding\"].apply(lambda x: cosine_similarity(query_embedding, x))\n",
    "\n",
    "# Get top 3 most relevant chunks\n",
    "top_chunks = df_embeddings.sort_values(by=\"similarity\", ascending=False).head(3)\n",
    "\n",
    "# Retrieve corresponding text chunks\n",
    "retrieved_chunks = spark.sql(\"SELECT * FROM rag_chunks\").toPandas()\n",
    "retrieved_texts = retrieved_chunks[retrieved_chunks[\"id\"].isin(top_chunks[\"id\"])][\"text\"].tolist()\n",
    "\n",
    "print(\"🔎 Retrieved Chunks:\")\n",
    "for text in retrieved_texts:\n",
    "    print('******')\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad44dd9-3562-48e7-b467-b2b2e2febba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106b8b8fb68c4c67b98d8e46dc9d3c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3962873200264b5c8bc4c836341676ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628a1a9393b14f9aa120a7b0f097ed36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2490a270f2a45519f49ae7b33a6f1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6fdcbe07f54c88b250b6fe24adebe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebc7736dc0842bc8d64296b0838b3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd131ca63da4830a6df8592e260626c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a12b6c934bd48cea641311ca8ec0bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 AI Response:\n Context: has been observed to roll repeatedly for 2 m (6.6 ft), but specimens typically travel\nless than 1 m (3.3 ft).[51]\nThe mantis shrimp is eaten by a variety of cultures. In Japanese\ncuisine, the mantis shrimp species Oratosquilla oratoria,\ncalled shako ( 蝦蛄), is eaten boiled as a sushi topping, and\noccasionally raw as sashimi.\nMantis shrimps are also abundant along Vietnam's coast,\nknown in Vietnamese as bề bề, tôm tích or tôm tít.\nAdditionally,\nsome rock-burrowing species can do more damage to live rock\nthan the fishkeeper would prefer.\nThe live rock with mantis shrimp burrows is considered useful\nby some in the marine aquarium trade and is often collected. A\npiece of live rock not uncommonly conveys a live mantis\nshrimp into an aquarium.\nHowever, despite being\ncommon, they are poorly understood, as many species spend\nmost of their lives sheltering in burrows and holes.[4]\nCalled \"sea locusts\" by ancient Assyrians, \"prawn killers\" in\nAustralia,[5] and now sometimes referred to as \"thumb\nsplitters\" due to their ability to inflict painful wounds if\nhandled incautiously[6] mantis shrimp have powerful\nraptorial appendages that are used to attack and kill prey\neither by spearing, stunning, or dismembering.\n\nQuestion: What does mantis shrimp eat?\nAnswer:fish\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load Hugging Face LLM \n",
    "llm_pipeline = pipeline(\"text-generation\", model=\"facebook/opt-1.3b\")\n",
    "# very small model-> facebook/opt-6.7b is better but may crash the kernel \n",
    "\n",
    "# Generate response based on retrieved context\n",
    "context = \"\\n\".join(retrieved_texts)\n",
    "prompt = f\"Context: {context}\\n\\nQuestion: {query_text}\\nAnswer:\"\n",
    "\n",
    "response = llm_pipeline(prompt, max_length=150, num_return_sequences=1)\n",
    "print(\"💡 AI Response:\\n\", response[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5af9bd-dd0d-4692-979d-22240cd50e6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below code is for high level understanding and WILL NOT EXECUTE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "884ac474-c09a-428a-bafc-82682b866ae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/databricks/sdk/service/jobs.py:60: SyntaxWarning: invalid escape sequence '\\.'\n  \"\"\"The sequence number of this run attempt for a triggered job run. The initial attempt of a run\n/databricks/python/lib/python3.12/site-packages/databricks/sdk/service/jobs.py:2570: SyntaxWarning: invalid escape sequence '\\.'\n  \"\"\"The sequence number of this run attempt for a triggered job run. The initial attempt of a run\n/databricks/python/lib/python3.12/site-packages/databricks/sdk/service/jobs.py:3431: SyntaxWarning: invalid escape sequence '\\.'\n  \"\"\"The sequence number of this run attempt for a triggered job run. The initial attempt of a run\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/utils/models.py:32: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n  latest = client.get_latest_versions(name, None if stage is None else [stage])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mRestException\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7838288942542868>, line 19\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Load Databricks Model for Embeddings\u001B[39;00m\n",
       "\u001B[1;32m     18\u001B[0m embedding_model_uri \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels:/your_embedding_model/production\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 19\u001B[0m embedding_model \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mpyfunc\u001B[38;5;241m.\u001B[39mload_model(embedding_model_uri)\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_embeddings\u001B[39m(texts):\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Generate embeddings using Databricks MLflow Model.\"\"\"\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/tracing/provider.py:237\u001B[0m, in \u001B[0;36mtrace_disabled.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    235\u001B[0m disable()\n",
       "\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 237\u001B[0m     is_func_called, result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m    239\u001B[0m     enable()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/pyfunc/__init__.py:993\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(model_uri, suppress_warnings, dst_path, model_config)\u001B[0m\n",
       "\u001B[1;32m    989\u001B[0m         entity_list\u001B[38;5;241m.\u001B[39mappend(Entity(job\u001B[38;5;241m=\u001B[39mjob_entity))\n",
       "\u001B[1;32m    991\u001B[0m     lineage_header_info \u001B[38;5;241m=\u001B[39m LineageHeaderInfo(entities\u001B[38;5;241m=\u001B[39mentity_list) \u001B[38;5;28;01mif\u001B[39;00m entity_list \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 993\u001B[0m local_path \u001B[38;5;241m=\u001B[39m _download_artifact_from_uri(\n",
       "\u001B[1;32m    994\u001B[0m     artifact_uri\u001B[38;5;241m=\u001B[39mmodel_uri, output_path\u001B[38;5;241m=\u001B[39mdst_path, lineage_header_info\u001B[38;5;241m=\u001B[39mlineage_header_info\n",
       "\u001B[1;32m    995\u001B[0m )\n",
       "\u001B[1;32m    997\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m suppress_warnings:\n",
       "\u001B[1;32m    998\u001B[0m     model_requirements \u001B[38;5;241m=\u001B[39m _get_pip_requirements_from_model_path(local_path)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/tracking/artifact_utils.py:108\u001B[0m, in \u001B[0;36m_download_artifact_from_uri\u001B[0;34m(artifact_uri, output_path, lineage_header_info)\u001B[0m\n",
       "\u001B[1;32m    100\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    101\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n",
       "\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m    artifact_uri: The *absolute* URI of the artifact to download.\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    105\u001B[0m \u001B[38;5;124;03m    lineage_header_info: The model lineage header info to be consumed by lineage services.\u001B[39;00m\n",
       "\u001B[1;32m    106\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    107\u001B[0m root_uri, artifact_path \u001B[38;5;241m=\u001B[39m _get_root_uri_and_artifact_path(artifact_uri)\n",
       "\u001B[0;32m--> 108\u001B[0m repo \u001B[38;5;241m=\u001B[39m get_artifact_repository(artifact_uri\u001B[38;5;241m=\u001B[39mroot_uri)\n",
       "\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(repo, ModelsArtifactRepository):\n",
       "\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m repo\u001B[38;5;241m.\u001B[39mdownload_artifacts(\n",
       "\u001B[1;32m    112\u001B[0m         artifact_path\u001B[38;5;241m=\u001B[39martifact_path,\n",
       "\u001B[1;32m    113\u001B[0m         dst_path\u001B[38;5;241m=\u001B[39moutput_path,\n",
       "\u001B[1;32m    114\u001B[0m         lineage_header_info\u001B[38;5;241m=\u001B[39mlineage_header_info,\n",
       "\u001B[1;32m    115\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/artifact_repository_registry.py:133\u001B[0m, in \u001B[0;36mget_artifact_repository\u001B[0;34m(artifact_uri)\u001B[0m\n",
       "\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_artifact_repository\u001B[39m(artifact_uri: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ArtifactRepository:\n",
       "\u001B[1;32m    121\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    122\u001B[0m \u001B[38;5;124;03m    Get an artifact repository from the registry based on the scheme of artifact_uri\u001B[39;00m\n",
       "\u001B[1;32m    123\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    131\u001B[0m \u001B[38;5;124;03m        requirements.\u001B[39;00m\n",
       "\u001B[1;32m    132\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _artifact_repository_registry\u001B[38;5;241m.\u001B[39mget_artifact_repository(artifact_uri)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/artifact_repository_registry.py:78\u001B[0m, in \u001B[0;36mArtifactRepositoryRegistry.get_artifact_repository\u001B[0;34m(self, artifact_uri)\u001B[0m\n",
       "\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m repository \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m     74\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\n",
       "\u001B[1;32m     75\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not find a registered artifact repository for: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00martifact_uri\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     76\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCurrently registered schemes are: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_registry\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     77\u001B[0m     )\n",
       "\u001B[0;32m---> 78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m repository(artifact_uri)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/models_artifact_repo.py:51\u001B[0m, in \u001B[0;36mModelsArtifactRepository.__init__\u001B[0;34m(self, artifact_uri)\u001B[0m\n",
       "\u001B[1;32m     48\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_version \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo\u001B[38;5;241m.\u001B[39mmodel_version\n",
       "\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_using_databricks_registry(artifact_uri):\n",
       "\u001B[1;32m     50\u001B[0m     \u001B[38;5;66;03m# Use the DatabricksModelsArtifactRepository if a databricks profile is being used.\u001B[39;00m\n",
       "\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo \u001B[38;5;241m=\u001B[39m DatabricksModelsArtifactRepository(artifact_uri)\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo\u001B[38;5;241m.\u001B[39mmodel_name\n",
       "\u001B[1;32m     53\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_version \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo\u001B[38;5;241m.\u001B[39mmodel_version\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/databricks_models_artifact_repo.py:70\u001B[0m, in \u001B[0;36mDatabricksModelsArtifactRepository.__init__\u001B[0;34m(self, artifact_uri)\u001B[0m\n",
       "\u001B[1;32m     68\u001B[0m warn_on_deprecated_cross_workspace_registry_uri(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatabricks_profile_uri)\n",
       "\u001B[1;32m     69\u001B[0m client \u001B[38;5;241m=\u001B[39m MlflowClient(registry_uri\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatabricks_profile_uri)\n",
       "\u001B[0;32m---> 70\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_version \u001B[38;5;241m=\u001B[39m get_model_name_and_version(client, artifact_uri)\n",
       "\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m# Use an isolated thread pool executor for chunk uploads/downloads to avoid a deadlock\u001B[39;00m\n",
       "\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# caused by waiting for a chunk-upload/download task within a file-upload/download task.\u001B[39;00m\n",
       "\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# See https://superfastpython.com/threadpoolexecutor-deadlock/#Deadlock_1_Submit_and_Wait_for_a_Task_Within_a_Task\u001B[39;00m\n",
       "\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# for more details\u001B[39;00m\n",
       "\u001B[1;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunk_thread_pool \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_thread_pool()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/utils/models.py:94\u001B[0m, in \u001B[0;36mget_model_name_and_version\u001B[0;34m(client, models_uri)\u001B[0m\n",
       "\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_alias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_name, client\u001B[38;5;241m.\u001B[39mget_model_version_by_alias(model_name, model_alias)\u001B[38;5;241m.\u001B[39mversion\n",
       "\u001B[0;32m---> 94\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model_name, \u001B[38;5;28mstr\u001B[39m(_get_latest_model_version(client, model_name, model_stage))\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/utils/models.py:32\u001B[0m, in \u001B[0;36m_get_latest_model_version\u001B[0;34m(client, name, stage)\u001B[0m\n",
       "\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_latest_model_version\u001B[39m(client, name, stage):\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;124;03m    Returns the latest version of the stage if stage is not None. Otherwise return the latest of all\u001B[39;00m\n",
       "\u001B[1;32m     30\u001B[0m \u001B[38;5;124;03m    versions.\u001B[39;00m\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m---> 32\u001B[0m     latest \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mget_latest_versions(name, \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m stage \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m [stage])\n",
       "\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(latest) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[1;32m     34\u001B[0m         stage_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stage \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m and stage \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/annotations.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecated_decorator.<locals>.deprecated_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    142\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n",
       "\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeprecated_func\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[1;32m    144\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(notice, category\u001B[38;5;241m=\u001B[39m\u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
       "\u001B[0;32m--> 145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/tracking/client.py:3468\u001B[0m, in \u001B[0;36mMlflowClient.get_latest_versions\u001B[0;34m(self, name, stages)\u001B[0m\n",
       "\u001B[1;32m   3395\u001B[0m \u001B[38;5;129m@deprecated\u001B[39m(since\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.9.0\u001B[39m\u001B[38;5;124m\"\u001B[39m, impact\u001B[38;5;241m=\u001B[39m_STAGES_DEPRECATION_WARNING)\n",
       "\u001B[1;32m   3396\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_latest_versions\u001B[39m(\n",
       "\u001B[1;32m   3397\u001B[0m     \u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m, stages: Optional[List[\u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   3398\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[ModelVersion]:\n",
       "\u001B[1;32m   3399\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   3400\u001B[0m \u001B[38;5;124;03m    Latest version models for each requests stage. If no ``stages`` provided, returns the\u001B[39;00m\n",
       "\u001B[1;32m   3401\u001B[0m \u001B[38;5;124;03m    latest version for each stage.\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3466\u001B[0m \u001B[38;5;124;03m        current_stage: None\u001B[39;00m\n",
       "\u001B[1;32m   3467\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3468\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_registry_client()\u001B[38;5;241m.\u001B[39mget_latest_versions(name, stages)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/tracking/_model_registry/client.py:152\u001B[0m, in \u001B[0;36mModelRegistryClient.get_latest_versions\u001B[0;34m(self, name, stages)\u001B[0m\n",
       "\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_latest_versions\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, stages\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n",
       "\u001B[1;32m    140\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Latest version models for each requests stage. If no ``stages`` provided, returns the\u001B[39;00m\n",
       "\u001B[1;32m    141\u001B[0m \u001B[38;5;124;03m    latest version for each stage.\u001B[39;00m\n",
       "\u001B[1;32m    142\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    150\u001B[0m \n",
       "\u001B[1;32m    151\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstore\u001B[38;5;241m.\u001B[39mget_latest_versions(name, stages)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/model_registry/rest_store.py:193\u001B[0m, in \u001B[0;36mRestStore.get_latest_versions\u001B[0;34m(self, name, stages)\u001B[0m\n",
       "\u001B[1;32m    180\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    181\u001B[0m \u001B[38;5;124;03mLatest version models for each requested stage. If no ``stages`` argument is provided,\u001B[39;00m\n",
       "\u001B[1;32m    182\u001B[0m \u001B[38;5;124;03mreturns the latest version for each stage.\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    190\u001B[0m \u001B[38;5;124;03m    List of :py:class:`mlflow.entities.model_registry.ModelVersion` objects.\u001B[39;00m\n",
       "\u001B[1;32m    191\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    192\u001B[0m req_body \u001B[38;5;241m=\u001B[39m message_to_json(GetLatestVersions(name\u001B[38;5;241m=\u001B[39mname, stages\u001B[38;5;241m=\u001B[39mstages))\n",
       "\u001B[0;32m--> 193\u001B[0m response_proto \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_endpoint(GetLatestVersions, req_body, call_all_endpoints\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m    194\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\n",
       "\u001B[1;32m    195\u001B[0m     ModelVersion\u001B[38;5;241m.\u001B[39mfrom_proto(model_version)\n",
       "\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m model_version \u001B[38;5;129;01min\u001B[39;00m response_proto\u001B[38;5;241m.\u001B[39mmodel_versions\n",
       "\u001B[1;32m    197\u001B[0m ]\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/model_registry/base_rest_store.py:39\u001B[0m, in \u001B[0;36mBaseRestStore._call_endpoint\u001B[0;34m(self, api, json_body, call_all_endpoints, extra_headers)\u001B[0m\n",
       "\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m call_all_endpoints:\n",
       "\u001B[1;32m     38\u001B[0m     endpoints \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_all_endpoints_from_method(api)\n",
       "\u001B[0;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m call_endpoints(\n",
       "\u001B[1;32m     40\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_host_creds(), endpoints, json_body, response_proto, extra_headers\n",
       "\u001B[1;32m     41\u001B[0m     )\n",
       "\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m     43\u001B[0m     endpoint, method \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_endpoint_from_method(api)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:378\u001B[0m, in \u001B[0;36mcall_endpoints\u001B[0;34m(host_creds, endpoints, json_body, response_proto, extra_headers)\u001B[0m\n",
       "\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RestException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    377\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39merror_code \u001B[38;5;241m!=\u001B[39m ErrorCode\u001B[38;5;241m.\u001B[39mName(ENDPOINT_NOT_FOUND) \u001B[38;5;129;01mor\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(endpoints) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\u001B[0;32m--> 378\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:373\u001B[0m, in \u001B[0;36mcall_endpoints\u001B[0;34m(host_creds, endpoints, json_body, response_proto, extra_headers)\u001B[0m\n",
       "\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (endpoint, method) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(endpoints):\n",
       "\u001B[1;32m    372\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 373\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m call_endpoint(\n",
       "\u001B[1;32m    374\u001B[0m             host_creds, endpoint, method, json_body, response_proto, extra_headers\n",
       "\u001B[1;32m    375\u001B[0m         )\n",
       "\u001B[1;32m    376\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m RestException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    377\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39merror_code \u001B[38;5;241m!=\u001B[39m ErrorCode\u001B[38;5;241m.\u001B[39mName(ENDPOINT_NOT_FOUND) \u001B[38;5;129;01mor\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(endpoints) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:362\u001B[0m, in \u001B[0;36mcall_endpoint\u001B[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001B[0m\n",
       "\u001B[1;32m    359\u001B[0m     call_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m json_body\n",
       "\u001B[1;32m    360\u001B[0m     response \u001B[38;5;241m=\u001B[39m http_request(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcall_kwargs)\n",
       "\u001B[0;32m--> 362\u001B[0m response \u001B[38;5;241m=\u001B[39m verify_rest_response(response, endpoint)\n",
       "\u001B[1;32m    363\u001B[0m js_dict \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mtext)\n",
       "\u001B[1;32m    364\u001B[0m parse_dict(js_dict\u001B[38;5;241m=\u001B[39mjs_dict, message\u001B[38;5;241m=\u001B[39mresponse_proto)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:232\u001B[0m, in \u001B[0;36mverify_rest_response\u001B[0;34m(response, endpoint)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m200\u001B[39m:\n",
       "\u001B[1;32m    231\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _can_parse_as_json_object(response\u001B[38;5;241m.\u001B[39mtext):\n",
       "\u001B[0;32m--> 232\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m RestException(json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mtext))\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    234\u001B[0m         base_msg \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m    235\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAPI request to endpoint \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mendpoint\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    236\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfailed with error code \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m != 200\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    237\u001B[0m         )\n",
       "\n",
       "\u001B[0;31mRestException\u001B[0m: RESOURCE_DOES_NOT_EXIST: RegisteredModel 'your_embedding_model' does not exist. It might have been deleted."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "RestException",
        "evalue": "RESOURCE_DOES_NOT_EXIST: RegisteredModel 'your_embedding_model' does not exist. It might have been deleted."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>RestException</span>: RESOURCE_DOES_NOT_EXIST: RegisteredModel 'your_embedding_model' does not exist. It might have been deleted."
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mRestException\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-7838288942542868>, line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Load Databricks Model for Embeddings\u001B[39;00m\n\u001B[1;32m     18\u001B[0m embedding_model_uri \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels:/your_embedding_model/production\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 19\u001B[0m embedding_model \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mpyfunc\u001B[38;5;241m.\u001B[39mload_model(embedding_model_uri)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_embeddings\u001B[39m(texts):\n\u001B[1;32m     22\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Generate embeddings using Databricks MLflow Model.\"\"\"\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/tracing/provider.py:237\u001B[0m, in \u001B[0;36mtrace_disabled.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    235\u001B[0m disable()\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 237\u001B[0m     is_func_called, result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    239\u001B[0m     enable()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/pyfunc/__init__.py:993\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(model_uri, suppress_warnings, dst_path, model_config)\u001B[0m\n\u001B[1;32m    989\u001B[0m         entity_list\u001B[38;5;241m.\u001B[39mappend(Entity(job\u001B[38;5;241m=\u001B[39mjob_entity))\n\u001B[1;32m    991\u001B[0m     lineage_header_info \u001B[38;5;241m=\u001B[39m LineageHeaderInfo(entities\u001B[38;5;241m=\u001B[39mentity_list) \u001B[38;5;28;01mif\u001B[39;00m entity_list \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 993\u001B[0m local_path \u001B[38;5;241m=\u001B[39m _download_artifact_from_uri(\n\u001B[1;32m    994\u001B[0m     artifact_uri\u001B[38;5;241m=\u001B[39mmodel_uri, output_path\u001B[38;5;241m=\u001B[39mdst_path, lineage_header_info\u001B[38;5;241m=\u001B[39mlineage_header_info\n\u001B[1;32m    995\u001B[0m )\n\u001B[1;32m    997\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m suppress_warnings:\n\u001B[1;32m    998\u001B[0m     model_requirements \u001B[38;5;241m=\u001B[39m _get_pip_requirements_from_model_path(local_path)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/tracking/artifact_utils.py:108\u001B[0m, in \u001B[0;36m_download_artifact_from_uri\u001B[0;34m(artifact_uri, output_path, lineage_header_info)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m    artifact_uri: The *absolute* URI of the artifact to download.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;124;03m    lineage_header_info: The model lineage header info to be consumed by lineage services.\u001B[39;00m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    107\u001B[0m root_uri, artifact_path \u001B[38;5;241m=\u001B[39m _get_root_uri_and_artifact_path(artifact_uri)\n\u001B[0;32m--> 108\u001B[0m repo \u001B[38;5;241m=\u001B[39m get_artifact_repository(artifact_uri\u001B[38;5;241m=\u001B[39mroot_uri)\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(repo, ModelsArtifactRepository):\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m repo\u001B[38;5;241m.\u001B[39mdownload_artifacts(\n\u001B[1;32m    112\u001B[0m         artifact_path\u001B[38;5;241m=\u001B[39martifact_path,\n\u001B[1;32m    113\u001B[0m         dst_path\u001B[38;5;241m=\u001B[39moutput_path,\n\u001B[1;32m    114\u001B[0m         lineage_header_info\u001B[38;5;241m=\u001B[39mlineage_header_info,\n\u001B[1;32m    115\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/artifact_repository_registry.py:133\u001B[0m, in \u001B[0;36mget_artifact_repository\u001B[0;34m(artifact_uri)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_artifact_repository\u001B[39m(artifact_uri: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ArtifactRepository:\n\u001B[1;32m    121\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;124;03m    Get an artifact repository from the registry based on the scheme of artifact_uri\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;124;03m        requirements.\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _artifact_repository_registry\u001B[38;5;241m.\u001B[39mget_artifact_repository(artifact_uri)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/artifact_repository_registry.py:78\u001B[0m, in \u001B[0;36mArtifactRepositoryRegistry.get_artifact_repository\u001B[0;34m(self, artifact_uri)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m repository \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\n\u001B[1;32m     75\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not find a registered artifact repository for: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00martifact_uri\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     76\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCurrently registered schemes are: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_registry\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     77\u001B[0m     )\n\u001B[0;32m---> 78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m repository(artifact_uri)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/models_artifact_repo.py:51\u001B[0m, in \u001B[0;36mModelsArtifactRepository.__init__\u001B[0;34m(self, artifact_uri)\u001B[0m\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_version \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo\u001B[38;5;241m.\u001B[39mmodel_version\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_using_databricks_registry(artifact_uri):\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;66;03m# Use the DatabricksModelsArtifactRepository if a databricks profile is being used.\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo \u001B[38;5;241m=\u001B[39m DatabricksModelsArtifactRepository(artifact_uri)\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo\u001B[38;5;241m.\u001B[39mmodel_name\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_version \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo\u001B[38;5;241m.\u001B[39mmodel_version\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/databricks_models_artifact_repo.py:70\u001B[0m, in \u001B[0;36mDatabricksModelsArtifactRepository.__init__\u001B[0;34m(self, artifact_uri)\u001B[0m\n\u001B[1;32m     68\u001B[0m warn_on_deprecated_cross_workspace_registry_uri(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatabricks_profile_uri)\n\u001B[1;32m     69\u001B[0m client \u001B[38;5;241m=\u001B[39m MlflowClient(registry_uri\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatabricks_profile_uri)\n\u001B[0;32m---> 70\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_version \u001B[38;5;241m=\u001B[39m get_model_name_and_version(client, artifact_uri)\n\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m# Use an isolated thread pool executor for chunk uploads/downloads to avoid a deadlock\u001B[39;00m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# caused by waiting for a chunk-upload/download task within a file-upload/download task.\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# See https://superfastpython.com/threadpoolexecutor-deadlock/#Deadlock_1_Submit_and_Wait_for_a_Task_Within_a_Task\u001B[39;00m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# for more details\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunk_thread_pool \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_thread_pool()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/utils/models.py:94\u001B[0m, in \u001B[0;36mget_model_name_and_version\u001B[0;34m(client, models_uri)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_alias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_name, client\u001B[38;5;241m.\u001B[39mget_model_version_by_alias(model_name, model_alias)\u001B[38;5;241m.\u001B[39mversion\n\u001B[0;32m---> 94\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model_name, \u001B[38;5;28mstr\u001B[39m(_get_latest_model_version(client, model_name, model_stage))\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/utils/models.py:32\u001B[0m, in \u001B[0;36m_get_latest_model_version\u001B[0;34m(client, name, stage)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_latest_model_version\u001B[39m(client, name, stage):\n\u001B[1;32m     28\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;124;03m    Returns the latest version of the stage if stage is not None. Otherwise return the latest of all\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;124;03m    versions.\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m     latest \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mget_latest_versions(name, \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m stage \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m [stage])\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(latest) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     34\u001B[0m         stage_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stage \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m and stage \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/annotations.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecated_decorator.<locals>.deprecated_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeprecated_func\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    144\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(notice, category\u001B[38;5;241m=\u001B[39m\u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m--> 145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/tracking/client.py:3468\u001B[0m, in \u001B[0;36mMlflowClient.get_latest_versions\u001B[0;34m(self, name, stages)\u001B[0m\n\u001B[1;32m   3395\u001B[0m \u001B[38;5;129m@deprecated\u001B[39m(since\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.9.0\u001B[39m\u001B[38;5;124m\"\u001B[39m, impact\u001B[38;5;241m=\u001B[39m_STAGES_DEPRECATION_WARNING)\n\u001B[1;32m   3396\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_latest_versions\u001B[39m(\n\u001B[1;32m   3397\u001B[0m     \u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m, stages: Optional[List[\u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3398\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[ModelVersion]:\n\u001B[1;32m   3399\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3400\u001B[0m \u001B[38;5;124;03m    Latest version models for each requests stage. If no ``stages`` provided, returns the\u001B[39;00m\n\u001B[1;32m   3401\u001B[0m \u001B[38;5;124;03m    latest version for each stage.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3466\u001B[0m \u001B[38;5;124;03m        current_stage: None\u001B[39;00m\n\u001B[1;32m   3467\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3468\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_registry_client()\u001B[38;5;241m.\u001B[39mget_latest_versions(name, stages)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/tracking/_model_registry/client.py:152\u001B[0m, in \u001B[0;36mModelRegistryClient.get_latest_versions\u001B[0;34m(self, name, stages)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_latest_versions\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, stages\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    140\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Latest version models for each requests stage. If no ``stages`` provided, returns the\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;124;03m    latest version for each stage.\u001B[39;00m\n\u001B[1;32m    142\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    150\u001B[0m \n\u001B[1;32m    151\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstore\u001B[38;5;241m.\u001B[39mget_latest_versions(name, stages)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/model_registry/rest_store.py:193\u001B[0m, in \u001B[0;36mRestStore.get_latest_versions\u001B[0;34m(self, name, stages)\u001B[0m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;124;03mLatest version models for each requested stage. If no ``stages`` argument is provided,\u001B[39;00m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;124;03mreturns the latest version for each stage.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;124;03m    List of :py:class:`mlflow.entities.model_registry.ModelVersion` objects.\u001B[39;00m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    192\u001B[0m req_body \u001B[38;5;241m=\u001B[39m message_to_json(GetLatestVersions(name\u001B[38;5;241m=\u001B[39mname, stages\u001B[38;5;241m=\u001B[39mstages))\n\u001B[0;32m--> 193\u001B[0m response_proto \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_endpoint(GetLatestVersions, req_body, call_all_endpoints\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    194\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m    195\u001B[0m     ModelVersion\u001B[38;5;241m.\u001B[39mfrom_proto(model_version)\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m model_version \u001B[38;5;129;01min\u001B[39;00m response_proto\u001B[38;5;241m.\u001B[39mmodel_versions\n\u001B[1;32m    197\u001B[0m ]\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/model_registry/base_rest_store.py:39\u001B[0m, in \u001B[0;36mBaseRestStore._call_endpoint\u001B[0;34m(self, api, json_body, call_all_endpoints, extra_headers)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m call_all_endpoints:\n\u001B[1;32m     38\u001B[0m     endpoints \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_all_endpoints_from_method(api)\n\u001B[0;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m call_endpoints(\n\u001B[1;32m     40\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_host_creds(), endpoints, json_body, response_proto, extra_headers\n\u001B[1;32m     41\u001B[0m     )\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     43\u001B[0m     endpoint, method \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_endpoint_from_method(api)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:378\u001B[0m, in \u001B[0;36mcall_endpoints\u001B[0;34m(host_creds, endpoints, json_body, response_proto, extra_headers)\u001B[0m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    377\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39merror_code \u001B[38;5;241m!=\u001B[39m ErrorCode\u001B[38;5;241m.\u001B[39mName(ENDPOINT_NOT_FOUND) \u001B[38;5;129;01mor\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(endpoints) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 378\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:373\u001B[0m, in \u001B[0;36mcall_endpoints\u001B[0;34m(host_creds, endpoints, json_body, response_proto, extra_headers)\u001B[0m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (endpoint, method) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(endpoints):\n\u001B[1;32m    372\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 373\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m call_endpoint(\n\u001B[1;32m    374\u001B[0m             host_creds, endpoint, method, json_body, response_proto, extra_headers\n\u001B[1;32m    375\u001B[0m         )\n\u001B[1;32m    376\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m RestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    377\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39merror_code \u001B[38;5;241m!=\u001B[39m ErrorCode\u001B[38;5;241m.\u001B[39mName(ENDPOINT_NOT_FOUND) \u001B[38;5;129;01mor\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(endpoints) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:362\u001B[0m, in \u001B[0;36mcall_endpoint\u001B[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001B[0m\n\u001B[1;32m    359\u001B[0m     call_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m json_body\n\u001B[1;32m    360\u001B[0m     response \u001B[38;5;241m=\u001B[39m http_request(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcall_kwargs)\n\u001B[0;32m--> 362\u001B[0m response \u001B[38;5;241m=\u001B[39m verify_rest_response(response, endpoint)\n\u001B[1;32m    363\u001B[0m js_dict \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mtext)\n\u001B[1;32m    364\u001B[0m parse_dict(js_dict\u001B[38;5;241m=\u001B[39mjs_dict, message\u001B[38;5;241m=\u001B[39mresponse_proto)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:232\u001B[0m, in \u001B[0;36mverify_rest_response\u001B[0;34m(response, endpoint)\u001B[0m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[1;32m    231\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _can_parse_as_json_object(response\u001B[38;5;241m.\u001B[39mtext):\n\u001B[0;32m--> 232\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m RestException(json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mtext))\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    234\u001B[0m         base_msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    235\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAPI request to endpoint \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mendpoint\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    236\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfailed with error code \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m != 200\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    237\u001B[0m         )\n",
        "\u001B[0;31mRestException\u001B[0m: RESOURCE_DOES_NOT_EXIST: RegisteredModel 'your_embedding_model' does not exist. It might have been deleted."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WARNING: FOLLOWING CODE WILL NOT WORK AND THROW ERROR DUE TO UNITY CATALOG/PERMISSIONS \n",
    "\n",
    "# Part 1- building embeddings and vectors\n",
    "\n",
    "import mlflow\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Databricks Vector Search Client\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "# Define Unity Catalog schema and table\n",
    "catalog_name = \"your_catalog\"\n",
    "schema_name = \"your_schema\"\n",
    "vector_index_name = \"your_vector_index\"\n",
    "\n",
    "# Load Databricks Model for Embeddings\n",
    "embedding_model_uri = \"models:/your_embedding_model/production\"\n",
    "embedding_model = mlflow.pyfunc.load_model(embedding_model_uri)\n",
    "\n",
    "def generate_embeddings(texts):\n",
    "    \"\"\"Generate embeddings using Databricks MLflow Model.\"\"\"\n",
    "    return [embedding_model.predict([text])[0] for text in texts]\n",
    "\n",
    "# Create or load the Vector Search index\n",
    "try:\n",
    "    vsc.get_index(catalog_name, schema_name, vector_index_name)\n",
    "except Exception:\n",
    "    vsc.create_index(catalog_name, schema_name, vector_index_name, dimension=768)\n",
    "\n",
    "# Load dataset from Unity Catalog\n",
    "df = spark.read.table(f\"{catalog_name}.{schema_name}.your_text_table\")\n",
    "\n",
    "# Generate embeddings and store them in Vector Search\n",
    "text_data = df.select(\"text_column\").rdd.map(lambda row: row[0]).collect()\n",
    "embeddings = generate_embeddings(text_data)\n",
    "\n",
    "# Convert to DataFrame and store in Vector Search\n",
    "vector_df = spark.createDataFrame(\n",
    "    [(i, text, embedding) for i, (text, embedding) in enumerate(zip(text_data, embeddings))],\n",
    "    [\"id\", \"text\", \"embedding\"]\n",
    ")\n",
    "\n",
    "vector_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.{vector_index_name}\")\n",
    "\n",
    "print(\"Embeddings stored in Unity Catalog and Databricks Vector Search.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036a94ef-e987-4904-ae07-6073dcf91777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WARNING: FOLLOWING CODE WILL NOT WORK AND THROW ERROR DUE TO UNITY CATALOG/PERMISSIONS\n",
    "\n",
    "# Part 2- RAG \n",
    "\n",
    "# Load a Databricks-hosted LLM model for RAG\n",
    "llm_model_uri = \"models:/your_llm_model/production\"\n",
    "llm_model = mlflow.pyfunc.load_model(llm_model_uri)\n",
    "\n",
    "def retrieve_relevant_chunks(query, top_k=5):\n",
    "    \"\"\"Retrieve relevant text chunks using Databricks Vector Search.\"\"\"\n",
    "    query_embedding = embedding_model.predict([query])[0]\n",
    "    results = vsc.search(\n",
    "        catalog_name, schema_name, vector_index_name,\n",
    "        query_embedding, top_k=top_k\n",
    "    )\n",
    "    return [row[\"text\"] for row in results]\n",
    "\n",
    "def generate_response(query):\n",
    "    \"\"\"Generate response using retrieved chunks and LLM.\"\"\"\n",
    "    relevant_chunks = retrieve_relevant_chunks(query)\n",
    "    context = \"\\n\".join(relevant_chunks)\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    response = llm_model.predict([prompt])[0]\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "query = \"What is a Mantis Shrimp?\"\n",
    "response = generate_response(query)\n",
    "print(\"Generated Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08_RAGWithDatabricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}